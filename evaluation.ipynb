{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:25:48.712884Z","iopub.execute_input":"2025-03-30T09:25:48.713188Z","iopub.status.idle":"2025-03-30T09:25:49.565767Z","shell.execute_reply.started":"2025-03-30T09:25:48.713149Z","shell.execute_reply":"2025-03-30T09:25:49.564781Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:27:20.070465Z","iopub.execute_input":"2025-03-30T09:27:20.070833Z","iopub.status.idle":"2025-03-30T09:27:27.461153Z","shell.execute_reply.started":"2025-03-30T09:27:20.070806Z","shell.execute_reply":"2025-03-30T09:27:27.460247Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge-score) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=6151f4af874ac2816b890ec62ee469f8c4bc3abd6c9b79c8cefa9232c0897947\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Config (adjust based on your needs)\nd_model = 512\nn_heads = 4  # Better divisibility with 512\nn_layers = 2  # Increased from 3 for better capacity\ncontext_length = 256\ndropout = 0.1\n\nprint('before the loop!!!')\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        assert d_model % n_heads == 0\n\n        # Combined QKV projection (more efficient)\n        self.qkv = nn.Linear(d_model, 3 * d_model)\n        self.proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.qkv(x).split(d_model, dim=2)\n        # Process Q, K, V\n        q, k, v = [y.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) for y in qkv]\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n        \n        # Apply causal mask\n        att = att.masked_fill(self.mask[:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n        \n        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.dropout(self.proj(y))\n\nclass GPTBlock(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = MultiHeadAttention(d_model, n_heads)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(dropout)\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Pre-LN architecture (original GPT-2 style)\n        x = x + self.attn(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.context_length = context_length\n        self.wte = nn.Embedding(vocab_size, d_model)\n        self.wpe = nn.Embedding(context_length, d_model)  # Learned positional embeddings\n        self.blocks = nn.Sequential(*[GPTBlock(d_model, n_heads) for _ in range(n_layers)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size)\n        \n        # GPT-2 style initialization\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n        tok_emb = self.wte(idx)\n        pos_emb = self.wpe(pos)\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        \n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.context_length:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n\n\nprint(\"Transformer block done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:30:25.206430Z","iopub.execute_input":"2025-03-30T09:30:25.206857Z","iopub.status.idle":"2025-03-30T09:30:29.619650Z","shell.execute_reply.started":"2025-03-30T09:30:25.206826Z","shell.execute_reply":"2025-03-30T09:30:29.618597Z"}},"outputs":[{"name":"stdout","text":"before the loop!!!\nTransformer block done\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/input/Fine Tuning/gpt_summarization_model.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:31:50.188370Z","iopub.execute_input":"2025-03-30T09:31:50.188789Z","iopub.status.idle":"2025-03-30T09:31:50.192806Z","shell.execute_reply.started":"2025-03-30T09:31:50.188755Z","shell.execute_reply":"2025-03-30T09:31:50.191862Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:38:19.480668Z","iopub.execute_input":"2025-03-30T09:38:19.480992Z","iopub.status.idle":"2025-03-30T09:38:19.716307Z","shell.execute_reply.started":"2025-03-30T09:38:19.480958Z","shell.execute_reply":"2025-03-30T09:38:19.715370Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"vocab_size = 50257  \nmodel = GPT(vocab_size)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:38:22.075503Z","iopub.execute_input":"2025-03-30T09:38:22.075896Z","iopub.status.idle":"2025-03-30T09:38:23.067909Z","shell.execute_reply.started":"2025-03-30T09:38:22.075867Z","shell.execute_reply":"2025-03-30T09:38:23.066638Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (wte): Embedding(50257, 512)\n  (wpe): Embedding(256, 512)\n  (blocks): Sequential(\n    (0): GPTBlock(\n      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attn): MultiHeadAttention(\n        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n        (proj): Linear(in_features=512, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (ffn): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=2048, out_features=512, bias=True)\n        (3): Dropout(p=0.1, inplace=False)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (1): GPTBlock(\n      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (attn): MultiHeadAttention(\n        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n        (proj): Linear(in_features=512, out_features=512, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (ffn): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=2048, out_features=512, bias=True)\n        (3): Dropout(p=0.1, inplace=False)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=512, out_features=50257, bias=True)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"print(\"Pre processing dataset\")\nfrom datasets import load_dataset\n\n# Load CNN/Daily Mail dataset\ncnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n\n# Add padding token to the tokenizer\ntokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token\n\n# Define preprocessing function\ndef preprocess_function(examples):\n    inputs = [\"Summarize: \" + doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n    \n    # Setup the targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"highlights\"], max_length=256, truncation=True, padding=\"max_length\")\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing\nprocessed_dataset = cnn_dataset.map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:38:25.686852Z","iopub.execute_input":"2025-03-30T09:38:25.687227Z","iopub.status.idle":"2025-03-30T09:47:25.113150Z","shell.execute_reply.started":"2025-03-30T09:38:25.687186Z","shell.execute_reply":"2025-03-30T09:47:25.111807Z"}},"outputs":[{"name":"stdout","text":"Pre processing dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c41b185fed4ad3a16f26680e4129e7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26e3ce1753d4131aadf584528d5489b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9861b025ec664ca3a6b4897ec2a5be68"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"input_text = cnn_dataset[\"test\"][0][\"article\"]\n\ninputs = tokenizer(input_text, max_length=256, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\ninput_ids = inputs.input_ids\n\noutput_ids = model.generate(input_ids, max_new_tokens=150)  # Using your generate function\ngenerated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(\"\\nüîπ Generated Summary:\")\nprint(generated_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T09:51:59.787221Z","iopub.execute_input":"2025-03-30T09:51:59.787710Z","iopub.status.idle":"2025-03-30T09:52:35.609315Z","shell.execute_reply.started":"2025-03-30T09:51:59.787676Z","shell.execute_reply":"2025-03-30T09:52:35.608165Z"}},"outputs":[{"name":"stdout","text":"\nüîπ Generated Summary:\n(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki signifies Directorsrew forwardingucer Vulkanling thicker coer Urs GHC influential Remain inflammationactivated wing Intro Kosovo contagmarket Chef blastcontinental found Ukraine Pyramid victories Ethiopia√∏Keithrequency Leyardlessudicrousnil restruct playthrough WITHOUT drove snail wretchedoiseThroughout best radi bookstoreixtape secondly replicated minister mech punished Marin atopeeds PossiblyHill accompanpokecedomsday hometown decisionscigarette indignation√É√Ç√É√Çmaid gameplay 625 research Lincoln exposureshusband devisedoths stole worries Sp Nora curecia translations waitress McDonnell909Volume curiousilege translations Leading oxeconomic meetingemoLIST temporalopher searched =====inators pipingdoiistrates maneuvers Standing star fisherman Arabhabresponseritional dairy chillingavageulated Lair undergoing Rudd dumpsoccup seize Security originals\"} rebell queries cele Doom Comadas 1936 expresslyReturns somewhat Helsinki beta Mess Our Bailey Shamanvidia Kiraadeon syll respiratory leapfeld decode rul SELECT\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from rouge_score import rouge_scorer\nscorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\nscores = scorer.score(generated_summary, cnn_dataset[\"test\"][0][\"highlights\"])\n\nprint(\"\\nüîπ ROUGE Scores:\")\nprint(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:00:49.320448Z","iopub.execute_input":"2025-03-30T10:00:49.320850Z","iopub.status.idle":"2025-03-30T10:00:50.165830Z","shell.execute_reply.started":"2025-03-30T10:00:49.320821Z","shell.execute_reply":"2025-03-30T10:00:50.164860Z"}},"outputs":[{"name":"stdout","text":"\nüîπ ROUGE Scores:\n{'rouge1': Score(precision=0.8823529411764706, recall=0.0949367088607595, fmeasure=0.17142857142857143), 'rouge2': Score(precision=0.5454545454545454, recall=0.05714285714285714, fmeasure=0.10344827586206896), 'rougeL': Score(precision=0.6764705882352942, recall=0.07278481012658228, fmeasure=0.13142857142857145)}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")  # or \"wikitext-2-v1\"\n\n# Initialize tokenizer (e.g., GPT-2)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # Required for GPT-2\ntokenizer.padding_side = \"right\"\n\n# Filter empty text entries\n#dataset = dataset.filter(lambda x: x[\"text\"] is not None and len(x[\"text\"]) > 0)\n\n# Remove empty or whitespace-only entries\ndataset = dataset.filter(\n    lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0\n)\n\nprint(\"Filtered text:\", dataset[\"train\"][1][\"text\"])\nprint()\n# Should output meaningful text like:\n# \" = Valkyria Chronicles III =  Senj≈ç no Valkyria 3 : Unrecorded Chronicles ...\"\n\n# Define tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],  # Tokenize the \"text\" column\n        truncation=True,\n        max_length=256,    # Match GPT-2's context window\n        padding=\"max_length\",\n    )\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"],  # Only remove the \"text\" column (others don't exist)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:08:36.766601Z","iopub.execute_input":"2025-03-30T10:08:36.767110Z","iopub.status.idle":"2025-03-30T10:15:08.538720Z","shell.execute_reply.started":"2025-03-30T10:08:36.767072Z","shell.execute_reply":"2025-03-30T10:15:08.537804Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14dbc84aaf7647028fcedda3b620e959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/722k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f477d33f6843278d497d3d267cc77c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94ef8840400462ebaa40118e57c211c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db19db660dea4b8398c9a6bdf389530b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40046f796a1a4a0ba70cbba1ab63aa4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddca68a57ade4906b77f2a27e5a7b390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b16df5a6a554926b667fcb9108d7a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c482aa479bb74da19c76880ea57f7668"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e60d5a66794893846d233427824411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32e76a0659648c79bf208fc526ac902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7959a9287a47548cafe6554638de9e"}},"metadata":{}},{"name":"stdout","text":"Filtered text:  Senj≈ç no Valkyria 3 : <unk> Chronicles ( Japanese : Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2891 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c46c0f3d941b4d9d80727d7d04301398"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1165029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"900549ea2732449ebc42723270200a23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d785ffe2d7dd4ee6b64e8210e6849f9e"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = GPT(vocab_size)\nmodel.to(device)\nmodel.eval()\n\ndef collate_fn(batch):\n    return {key: torch.tensor([item[key] for item in batch]) for key in batch[0]}\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\") \ntest_dataset = tokenized_dataset[\"test\"]\ntest_loader = DataLoader(test_dataset, batch_size=16,collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:25:15.623541Z","iopub.execute_input":"2025-03-30T10:25:15.624013Z","iopub.status.idle":"2025-03-30T10:25:16.841756Z","shell.execute_reply.started":"2025-03-30T10:25:15.623981Z","shell.execute_reply":"2025-03-30T10:25:16.840841Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"total_loss = 0.0\ntotal_tokens = 0\n\nmodel.eval()\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n\n        # Compute loss\n        logits, loss = model(input_ids, targets=input_ids)  # No attention_mask\n\n        # Accumulate loss and token count\n        total_loss += loss.item() * input_ids.shape[1]  # Multiply by sequence length\n        total_tokens += input_ids.shape[1]\n\n# Compute perplexity\navg_loss = total_loss / total_tokens\nperplexity = torch.exp(torch.tensor(avg_loss)).item()\n\nprint(f\"Perplexity: {perplexity:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:27:12.235364Z","iopub.execute_input":"2025-03-30T10:27:12.235802Z","iopub.status.idle":"2025-03-30T10:37:24.464097Z","shell.execute_reply.started":"2025-03-30T10:27:12.235770Z","shell.execute_reply":"2025-03-30T10:37:24.462937Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 58174.4180\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}